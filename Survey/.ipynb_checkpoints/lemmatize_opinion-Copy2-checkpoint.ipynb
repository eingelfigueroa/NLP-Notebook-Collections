{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet \n",
    "# Create WordNetLemmatizer object \n",
    "\n",
    "sid = SentimentIntensityAnalyzer()\n",
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized POS Tagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    else:           \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('opinion_survey.csv')\n",
    "df = df.replace(np.nan, 'Neutral', regex=True) ### Some values are missing, use \"Neutral\" for missing values\n",
    "df = df.drop(df.columns[[0]],axis=1) ### Drop extra index\n",
    "test_df = df.iloc[0:, 0:] # Just for renaming purposes so that df variable can be used for testing\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stoplist = stopwords.words('english') + ['though']\n",
    "def remove_stopWords(w): \n",
    "    w = ' '.join(word for word in w.split() if word not in stoplist)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_range = len(test_df.columns) # number of columns\n",
    "\n",
    "for i in range(0,col_range):\n",
    "    col = test_df.columns[i] # The current column\n",
    "    test_df.loc[:,col] = test_df[col].apply(lambda x : str.lower(str(x))) ## To Lower Case\n",
    "    test_df.loc[:,col] = test_df[col].apply(lambda x : \" \".join(re.findall('[\\w]+',x))) # Remove Punctuations\n",
    "    test_df.loc[:,col] = test_df[col].apply(lambda x : remove_stopWords(x)) # Remove Stop words\n",
    "    \n",
    "    ##POS TAGGING\n",
    "    texts = test_df.loc[:,col].tolist()\n",
    "    tagged_texts = pos_tag_sents(map(word_tokenize, texts)) ### Tag every word in a row with POS\n",
    "    \n",
    "    ### Lemmatization\n",
    "    new = []\n",
    "    for i in tagged_texts:\n",
    "        #if len(i) > 0:\n",
    "        lemmatized_sentence = []\n",
    "        for word, tag in i:\n",
    "            tag = pos_tagger(tag) ### Convert POS Tag to known POS for simplification\n",
    "            if tag is None: \n",
    "    # if there is no available tag, append the token as is \n",
    "                lemmatized_sentence.append(word) \n",
    "            else:         \n",
    "    # else use the tag to lemmatize the token \n",
    "                lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "\n",
    "        lemmatized_sentence = \" \".join(lemmatized_sentence) \n",
    "        #print(lemmatized_sentence)\n",
    "        new.append(lemmatized_sentence)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "\n",
    "    test_df['POS'] = new ## Store tagged words\n",
    "    \n",
    "test_df = test_df.replace(r'^\\s*$', \"neutral\", regex=True) ## If row value is null, replace with neutral string\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.to_csv('lemmatized_opinion.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample POS Tags from first row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint \n",
    "\n",
    "pprint(tagged_texts[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem_df = pd.read_csv('lemmatized_opinion.csv')\n",
    "lem_df = lem_df.iloc[0:, 0:-1] # Remove last columns\n",
    "lem_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = []\n",
    "col_range = len(lem_df.columns) # number of columns\n",
    "\n",
    "for i in range(0,col_range):\n",
    "    col = test_df.columns[i] # The current column\n",
    "    test_df['scores'] = test_df[col].apply(lambda x: sid.polarity_scores(x)) ## Get polarity score of every Column\n",
    "    compound = test_df['scores'].apply(lambda score_dict: score_dict['compound']) ## Extract the compound from the results\n",
    "    test_df = test_df.drop('scores', 1) # Drop score DF in every iteration\n",
    "    compound = sum(compound)/140 # Get the mean compound of each columns\n",
    "    comp.append(compound) # Save mean and append to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_num = pd.read_csv('opinion_sentiment_score.csv')\n",
    "cor_num = df_num.iloc[0:, 0:7:6]\n",
    "cor_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ngram = pd.read_csv('ngram_sentiment_score.csv')\n",
    "#ngram_list = df_ngram.values.tolist()\n",
    "ngram_list = df_ngram.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cor_num = cor_num.rename({'Sentiment Score': 'Without Lemmanization'}, axis=1) \n",
    "cor_num[\"With Lemmanization\"] = comp\n",
    "cor_num[\"Lemmatized and Ngram\"] = ngram_list\n",
    "\n",
    "cor_num.style.set_caption('Comparison of Sentiment Score results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cor_num.to_csv(\"Sentiment_Comparison.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
