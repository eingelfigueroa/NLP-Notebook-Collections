{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 4.4MB/s eta 0:00:01     |██████████████████████▋         | 1.0MB 4.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Collecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/85/70c6602b078bd9e6f3da4f467047e906525c355a4dacd4f71b97a35d9897/joblib-1.0.1-py3-none-any.whl (303kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 7.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/ad/e0a6ea246c70fe549d8ef4a4632e66cccbdaab4830b04735f44144ed9308/regex-2020.11.13-cp36-cp36m-manylinux2010_x86_64.whl (666kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 22.2MB/s eta 0:00:01     |███████████████████████████████▌| 655kB 22.2MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/8c/f1035bd24b0e352ddba7be320abc1603fc4c9976fcda6971ed287be59164/tqdm-4.58.0-py2.py3-none-any.whl (73kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 22.9MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: joblib, regex, tqdm, nltk\n",
      "Successfully installed joblib-1.0.1 nltk-3.5 regex-2020.11.13 tqdm-4.58.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "nltk.download('wordnet') \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('averaged_perceptron_tagger') \n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import wordnet \n",
    "# Create WordNetLemmatizer object \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pos_tagger(nltk_tag): \n",
    "    if nltk_tag.startswith('J'): \n",
    "        return wordnet.ADJ \n",
    "    elif nltk_tag.startswith('V'): \n",
    "        return wordnet.VERB \n",
    "    elif nltk_tag.startswith('N'): \n",
    "        return wordnet.NOUN \n",
    "    elif nltk_tag.startswith('R'): \n",
    "        return wordnet.ADV \n",
    "    else:           \n",
    "        return None\n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'DT'), ('cat', 'NN'), ('is', 'VBZ'), ('sitting', 'VBG'), ('with', 'IN'), ('the', 'DT'), ('bats', 'NNS'), ('on', 'IN'), ('the', 'DT'), ('striped', 'JJ'), ('mat', 'NN'), ('under', 'IN'), ('many', 'JJ'), ('badly', 'RB'), ('flying', 'VBG'), ('geese', 'JJ')]\n",
      "the cat be sit with the bat on the striped mat under many badly fly geese\n"
     ]
    }
   ],
   "source": [
    "sentence = 'the cat is sitting with the bats on the striped mat under many badly flying geese'\n",
    "  \n",
    "# tokenize the sentence and find the POS tag for each token \n",
    "pos_tagged = nltk.pos_tag(nltk.word_tokenize(sentence))   \n",
    "  \n",
    "print(pos_tagged)\n",
    "wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged)) \n",
    "  \n",
    "lemmatized_sentence = [] \n",
    "for word, tag in wordnet_tagged: \n",
    "    if tag is None: \n",
    "        # if there is no available tag, append the token as is \n",
    "        lemmatized_sentence.append(word) \n",
    "    else:         \n",
    "        # else use the tag to lemmatize the token \n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, tag)) \n",
    "lemmatized_sentence = \" \".join(lemmatized_sentence) \n",
    "  \n",
    "print(lemmatized_sentence) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
