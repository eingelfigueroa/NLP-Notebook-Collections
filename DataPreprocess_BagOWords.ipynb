{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4MB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: click in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from nltk) (7.1.2)\n",
      "Collecting joblib (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/5b/bd0f0fb5564183884d8e35b81d06d7ec06a20d1a0c8b4c407f1554691dce/joblib-1.0.0-py3-none-any.whl (302kB)\n",
      "\u001b[K     |████████████████████████████████| 307kB 6.8MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/ad/e0a6ea246c70fe549d8ef4a4632e66cccbdaab4830b04735f44144ed9308/regex-2020.11.13-cp36-cp36m-manylinux2010_x86_64.whl (666kB)\n",
      "\u001b[K     |████████████████████████████████| 675kB 10.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tqdm (from nltk)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/02/8f8880a4fd6625461833abcf679d4c12a44c76f9925f92bf212bb6cefaad/tqdm-4.56.0-py2.py3-none-any.whl (72kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.5MB/s eta 0:00:011\n",
      "\u001b[?25hBuilding wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyterlab/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
      "Successfully built nltk\n",
      "Installing collected packages: joblib, regex, tqdm, nltk\n",
      "Successfully installed joblib-1.0.0 nltk-3.5 regex-2020.11.13 tqdm-4.56.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'T10.txt'\n",
    "with open(filepath) as f:\n",
    "    line = f.readline()\n",
    "    cnt = 1\n",
    "    fa = open('T10.csv', \"w\")\n",
    "    while line:\n",
    "        data = line.strip()\n",
    "        fa.write(data + \"\\n\")\n",
    "        line = f.readline()\n",
    "        cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Motivate students to achieve goal.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ability to explain difficult things in a simpl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Organization</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mostly highly recommended for all incoming fir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The best prof for freshmens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>She's perfect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>Im thankful that I enrolled your subject</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>She's so responsible and always go to class an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>354</th>\n",
       "      <td>Classroom management  needs to be improved</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>355</th>\n",
       "      <td>Flexible and easy to talk too..</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>356 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0\n",
       "0                   Motivate students to achieve goal.\n",
       "1    Ability to explain difficult things in a simpl...\n",
       "2                                         Organization\n",
       "3    Mostly highly recommended for all incoming fir...\n",
       "4                          The best prof for freshmens\n",
       "..                                                 ...\n",
       "351                                      She's perfect\n",
       "352           Im thankful that I enrolled your subject\n",
       "353  She's so responsible and always go to class an...\n",
       "354         Classroom management  needs to be improved\n",
       "355                    Flexible and easy to talk too..\n",
       "\n",
       "[356 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"T10.csv\",encoding = 'utf-8', sep='delimiter', header = None)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/e0/fa6326251692056dc880a64eb22117e03269906ba55a6864864d24ec8b4e/gensim-3.8.3-cp36-cp36m-manylinux1_x86_64.whl (24.2MB)\n",
      "\u001b[K     |████████████████████████████████| 24.2MB 316kB/s eta 0:00:011   |██████▍                         | 4.9MB 5.3MB/s eta 0:00:04     |████████████████████████████    | 21.3MB 2.8MB/s eta 0:00:02\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from gensim) (1.5.4)\n",
      "Requirement already satisfied: six>=1.5.0 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from gensim) (1.15.0)\n",
      "Collecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/54/01525817b6f31533d308968b814999f7e666b2234f39a55cbe5de7c1ff99/smart_open-4.1.2-py3-none-any.whl (111kB)\n",
      "\u001b[K     |████████████████████████████████| 112kB 8.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.3 in /home/jupyterlab/conda/envs/python/lib/python3.6/site-packages (from gensim) (1.19.4)\n",
      "Installing collected packages: smart-open, gensim\n",
      "Successfully installed gensim-3.8.3 smart-open-4.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "def preprocess(text):\n",
    "    clean_data = []\n",
    "    for x in (text[:][0]): #this is Df_pd for Df_np (text[:])\n",
    "        new_text = re.sub('<.*?>', '', x)   # remove HTML tags\n",
    "        new_text = re.sub(r'[^\\w\\s]', '', new_text) # remove punc.\n",
    "        new_text = re.sub(r'\\d+','',new_text)# remove numbers\n",
    "        new_text = new_text.lower() # lower case, .upper() for upper   \n",
    "        new_text = remove_stopwords(new_text)\n",
    "        if new_text != '':\n",
    "            clean_data.append(new_text)\n",
    "    return clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_s(sentences): # same can be achieved for words tokens\n",
    "    s_new = []\n",
    "    for sent in (sentences[:]): #For NumpY = sentences[:]\n",
    "        s_token = sent_tokenize(sent)\n",
    "        if s_token != '':\n",
    "            s_new.append(s_token)\n",
    "    return s_new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowball = SnowballStemmer(language = 'english')\n",
    "def stemming(words):\n",
    "    new = []\n",
    "    stem_words = [snowball.stem(x) for x in (words[:])]\n",
    "    new.append(stem_words)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization_w(words):\n",
    "    w_new = []\n",
    "    for w in (words[:][0]):  # for NumPy = words[:]\n",
    "        w_token = word_tokenize(w)\n",
    "        if w_token != '':\n",
    "            w_new.append(w_token)\n",
    "    return w_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jupyterlab/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_test = preprocess(dataframe) ## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_words = tokenization_s(clean_test) # word tokenization\n",
    "\n",
    "with open('clean_T10.txt', \"w\") as f:\n",
    "    for line in clean_words:\n",
    "        lmtzr = WordNetLemmatizer()\n",
    "        lemmatized = [[lmtzr.lemmatize(word) for word in word_tokenize(s)] for s in line]\n",
    "        results = \"\".join(map(str, lemmatized))\n",
    "        results = results.replace(',','')\n",
    "        f.write(str(results) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "### Run 3 times,\n",
    "with open('clean_T10.txt', \"r\") as f:\n",
    "    line = f.readline()\n",
    "    fa = open('cleaned_T10.csv', \"w\")\n",
    "    while line:\n",
    "        data = line.strip()\n",
    "        data = data.strip('[]')\n",
    "        data = data.replace(\"'\",\"\")\n",
    "        fa.write(data + \"\\n\")\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BAG OF WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyterlab/conda/envs/python/lib/python3.6/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                          motivate student achieve goal\n",
       "1      ability explain difficult thing simple way cre...\n",
       "2                                           organization\n",
       "3          highly recommended incoming year itcs student\n",
       "4                                     best prof freshman\n",
       "                             ...                        \n",
       "343                                         shes perfect\n",
       "344                         im thankful enrolled subject\n",
       "345               shes responsible class teach patiently\n",
       "346                   classroom management need improved\n",
       "347                                   flexible easy talk\n",
       "Name: 0, Length: 348, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_f = pd.read_csv(\"cleaned_T10.csv\",encoding = 'utf-8', sep='delimiter', header = None)\n",
    "data_f[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absence</th>\n",
       "      <th>absents</th>\n",
       "      <th>accept</th>\n",
       "      <th>achieve</th>\n",
       "      <th>acquired</th>\n",
       "      <th>action</th>\n",
       "      <th>activites</th>\n",
       "      <th>activity</th>\n",
       "      <th>...</th>\n",
       "      <th>working</th>\n",
       "      <th>world</th>\n",
       "      <th>wouldnt</th>\n",
       "      <th>wrong</th>\n",
       "      <th>yan</th>\n",
       "      <th>year</th>\n",
       "      <th>youtube</th>\n",
       "      <th>youve</th>\n",
       "      <th>yumie</th>\n",
       "      <th>zero</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>348 rows × 694 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     ability  able  absence  absents  accept  achieve  acquired  action  \\\n",
       "0          0     0        0        0       0        1         0       0   \n",
       "1          1     0        0        0       0        0         0       0   \n",
       "2          0     0        0        0       0        0         0       0   \n",
       "3          0     0        0        0       0        0         0       0   \n",
       "4          0     0        0        0       0        0         0       0   \n",
       "..       ...   ...      ...      ...     ...      ...       ...     ...   \n",
       "343        0     0        0        0       0        0         0       0   \n",
       "344        0     0        0        0       0        0         0       0   \n",
       "345        0     0        0        0       0        0         0       0   \n",
       "346        0     0        0        0       0        0         0       0   \n",
       "347        0     0        0        0       0        0         0       0   \n",
       "\n",
       "     activites  activity  ...  working  world  wouldnt  wrong  yan  year  \\\n",
       "0            0         0  ...        0      0        0      0    0     0   \n",
       "1            0         0  ...        0      0        0      0    0     0   \n",
       "2            0         0  ...        0      0        0      0    0     0   \n",
       "3            0         0  ...        0      0        0      0    0     1   \n",
       "4            0         0  ...        0      0        0      0    0     0   \n",
       "..         ...       ...  ...      ...    ...      ...    ...  ...   ...   \n",
       "343          0         0  ...        0      0        0      0    0     0   \n",
       "344          0         0  ...        0      0        0      0    0     0   \n",
       "345          0         0  ...        0      0        0      0    0     0   \n",
       "346          0         0  ...        0      0        0      0    0     0   \n",
       "347          0         0  ...        0      0        0      0    0     0   \n",
       "\n",
       "     youtube  youve  yumie  zero  \n",
       "0          0      0      0     0  \n",
       "1          0      0      0     0  \n",
       "2          0      0      0     0  \n",
       "3          0      0      0     0  \n",
       "4          0      0      0     0  \n",
       "..       ...    ...    ...   ...  \n",
       "343        0      0      0     0  \n",
       "344        0      0      0     0  \n",
       "345        0      0      0     0  \n",
       "346        0      0      0     0  \n",
       "347        0      0      0     0  \n",
       "\n",
       "[348 rows x 694 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "\n",
    "bow_lem_matrix = vectorizer.fit_transform(data_f[0])\n",
    "\n",
    "bow_df = pd.DataFrame(bow_lem_matrix.toarray())\n",
    "bow_df.columns = vectorizer.get_feature_names()\n",
    "\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bi-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiGram 348 have features\n",
      "     ability  ability explain  ability google  able  able best  able catch  \\\n",
      "0          0                0               0     0          0           0   \n",
      "1          1                1               0     0          0           0   \n",
      "2          0                0               0     0          0           0   \n",
      "3          0                0               0     0          0           0   \n",
      "4          0                0               0     0          0           0   \n",
      "..       ...              ...             ...   ...        ...         ...   \n",
      "343        0                0               0     0          0           0   \n",
      "344        0                0               0     0          0           0   \n",
      "345        0                0               0     0          0           0   \n",
      "346        0                0               0     0          0           0   \n",
      "347        0                0               0     0          0           0   \n",
      "\n",
      "     able catchup  able explain  absence  absence absence  ...  year life  \\\n",
      "0               0             0        0                0  ...          0   \n",
      "1               0             0        0                0  ...          0   \n",
      "2               0             0        0                0  ...          0   \n",
      "3               0             0        0                0  ...          0   \n",
      "4               0             0        0                0  ...          0   \n",
      "..            ...           ...      ...              ...  ...        ...   \n",
      "343             0             0        0                0  ...          0   \n",
      "344             0             0        0                0  ...          0   \n",
      "345             0             0        0                0  ...          0   \n",
      "346             0             0        0                0  ...          0   \n",
      "347             0             0        0                0  ...          0   \n",
      "\n",
      "     year taught  youtube  youtube channel  youve  youve given  yumie  \\\n",
      "0              0        0                0      0            0      0   \n",
      "1              0        0                0      0            0      0   \n",
      "2              0        0                0      0            0      0   \n",
      "3              0        0                0      0            0      0   \n",
      "4              0        0                0      0            0      0   \n",
      "..           ...      ...              ...    ...          ...    ...   \n",
      "343            0        0                0      0            0      0   \n",
      "344            0        0                0      0            0      0   \n",
      "345            0        0                0      0            0      0   \n",
      "346            0        0                0      0            0      0   \n",
      "347            0        0                0      0            0      0   \n",
      "\n",
      "     yumie honor  zero  zero knowledge  \n",
      "0              0     0               0  \n",
      "1              0     0               0  \n",
      "2              0     0               0  \n",
      "3              0     0               0  \n",
      "4              0     0               0  \n",
      "..           ...   ...             ...  \n",
      "343            0     0               0  \n",
      "344            0     0               0  \n",
      "345            0     0               0  \n",
      "346            0     0               0  \n",
      "347            0     0               0  \n",
      "\n",
      "[348 rows x 2038 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorizer_ng2 = CountVectorizer(ngram_range=(1, 2))\n",
    "ng2 = vectorizer_ng2.fit_transform(data_f[0])\n",
    "print(\"BiGram %i have features\" %(ng2.shape[0]))\n",
    "\n",
    "\n",
    "bgram = pd.DataFrame(ng2.toarray(), columns=vectorizer_ng2.get_feature_names())\n",
    "print(bgram)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ability  able  absence  absents  accept   achieve  acquired  action  \\\n",
      "0    0.000000   0.0      0.0      0.0     0.0  0.564617       0.0     0.0   \n",
      "1    0.372091   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "2    0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "3    0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "4    0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "..        ...   ...      ...      ...     ...       ...       ...     ...   \n",
      "343  0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "344  0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "345  0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "346  0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "347  0.000000   0.0      0.0      0.0     0.0  0.000000       0.0     0.0   \n",
      "\n",
      "     activites  activity  ...  working  world  wouldnt  wrong  yan      year  \\\n",
      "0          0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "1          0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "2          0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "3          0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.359479   \n",
      "4          0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "..         ...       ...  ...      ...    ...      ...    ...  ...       ...   \n",
      "343        0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "344        0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "345        0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "346        0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "347        0.0       0.0  ...      0.0    0.0      0.0    0.0  0.0  0.000000   \n",
      "\n",
      "     youtube  youve  yumie  zero  \n",
      "0        0.0    0.0    0.0   0.0  \n",
      "1        0.0    0.0    0.0   0.0  \n",
      "2        0.0    0.0    0.0   0.0  \n",
      "3        0.0    0.0    0.0   0.0  \n",
      "4        0.0    0.0    0.0   0.0  \n",
      "..       ...    ...    ...   ...  \n",
      "343      0.0    0.0    0.0   0.0  \n",
      "344      0.0    0.0    0.0   0.0  \n",
      "345      0.0    0.0    0.0   0.0  \n",
      "346      0.0    0.0    0.0   0.0  \n",
      "347      0.0    0.0    0.0   0.0  \n",
      "\n",
      "[348 rows x 683 columns]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf_vec = TfidfVectorizer(use_idf=True, \n",
    "                        smooth_idf=False,  \n",
    "                        ngram_range=(1,1),stop_words='english') # to use only  bigrams ngram_range=(2,2)\n",
    "\n",
    "tfd = tf_idf_vec.fit_transform(data_f[0])\n",
    "\n",
    "tf_idf_dataframe=pd.DataFrame(tfd.toarray(),columns=tf_idf_vec.get_feature_names())\n",
    "print(tf_idf_dataframe)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "conda-env-python-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
